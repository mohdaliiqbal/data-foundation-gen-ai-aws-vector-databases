{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e87dc259",
   "metadata": {},
   "source": [
    "# RAG using OpenSearch Neural Plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cfd51d",
   "metadata": {},
   "source": [
    "[Retrieval Augmented Generation](https://arxiv.org/abs/2005.11401) is a process that uses retrieval-based models such as Amazon Bedrock titan and generative models such as Anthropic Claude to enhance natural language generation by retrieving relevant information and incorporating it into the generation process. \n",
    "\n",
    "Much like the previous lab we are going to be writing a simple RAG application code that allows user to ask questions about various wines so they can make a purchasing decision. In this lab however we will use OpenSearch neural plugin that makes it simple to work with vector embeddings by creating embeddings for indexed data at indexing time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571c4d1e",
   "metadata": {},
   "source": [
    "## 1. Lab Pre-requisites\n",
    "\n",
    "#### a. Download and install python dependencies\n",
    "\n",
    "For this notebook we require the use of a few libraries. We'll use the Python clients for OpenSearch and SageMaker, and Python frameworks for text embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd01d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opensearch-py-ml accelerate tqdm --quiet\n",
    "!pip install sagemaker --upgrade --quiet\n",
    "!pip install requests_aws4auth --quiet\n",
    "!pip install alive-progress --quiet\n",
    "!pip install deprecated --quiet\n",
    "\n",
    "#OpenSearch Python SDK\n",
    "!pip install opensearch_py  --quiet\n",
    "#Progress bar for for loop\n",
    "!pip install alive-progress  --quiet\n",
    "\n",
    "# As in the previous modules, let's import PyTorch and confirm that the latest version of PyTorch is running. \n",
    "# The version should already be 1.13.1 or higher. If not, we will restart the kernel.\n",
    "\n",
    "import torch\n",
    "pytorch_version = torch.__version__\n",
    "print( f\"Pytorch version: {pytorch_version}\")\n",
    "\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "    \n",
    "if pytorch_version.startswith('1.1'):\n",
    "    from IPython.display import display_html\n",
    "    restartkernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa614bc",
   "metadata": {},
   "source": [
    "#### b. Import libraries & initialize resource information\n",
    "The line below will import all the relevant libraries and modules used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1688f4e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sagemaker\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from sagemaker import get_execution_role\n",
    "import random \n",
    "import string\n",
    "import s3fs\n",
    "from urllib.parse import urlparse\n",
    "from IPython.display import display, HTML\n",
    "from alive_progress import alive_bar\n",
    "from opensearch_py_ml.ml_commons import MLCommonClient\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import requests "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3459fff5",
   "metadata": {},
   "source": [
    "#### Get CloudFormation stack output variables\n",
    "\n",
    "We have preconfigured a few resources by creating a cloudformation stack in the account. Information of these resources will be used within this lab. We are going to load some of the information variables here.\n",
    "\n",
    "You can ignore any \"PythonDeprecationWarning\" warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc45a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Boto3 session\n",
    "session = boto3.Session()\n",
    "\n",
    "# Get the account id\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# Get the current region\n",
    "region = session.region_name\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "# Method to obtain output variables from Cloudformation stack. \n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "## Setup variables to use for the rest of the demo\n",
    "cloudformation_stack_name = \"advanced-opensearch-rag\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "aos_host = outputs['OpenSearchDomainEndpoint']\n",
    "s3_bucket = outputs['s3BucketTraining']\n",
    "bedrock_inf_iam_role = outputs['BedrockBatchInferenceRole']\n",
    "bedrock_inf_iam_role_arn = outputs['BedrockBatchInferenceRoleArn']\n",
    "sagemaker_notebook_url = outputs['SageMakerNotebookURL']\n",
    "\n",
    "# We will just print all the variables so you can easily copy if needed.\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6607721",
   "metadata": {},
   "source": [
    "## 2. Prepare data\n",
    "This lab combines semantic search with a generative model to present the retrieved data to the user . Below is a dataset of wine reviews, we'll sample this data set to recommend wines that resemble the user provided description.\n",
    "\n",
    "#### Mandatory steps to download the data manually\n",
    "Within these labs you will need to download the dataset from various sources. One is Kaggle (You will need to create a free account):\n",
    "https://www.kaggle.com/datasets/christopheiv/winemagdata130k?select=winemag-data-130k-v2.json\n",
    "\n",
    "Click **Download** button on the dataset page above. Once downloaded in your laptop, you will resume with following steps.\n",
    "\n",
    "1. Execute the following cell to get URL to SageMaker Notebook. Click the URL to open sagemaker notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9fc342",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = f'<a href=\"{sagemaker_notebook_url}\" target=\"_blank\">Sagemaker notebook URL</a>'\n",
    "display(HTML(link))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a216887",
   "metadata": {},
   "source": [
    "2. Browse to the `retrieval-augment-generation` directory\n",
    "3. Click \"Upload\" to upload the zip downloaded from Kaggle\n",
    "4. Click \"New\" -> \"Terminal\" to open a terminal window\n",
    "5. Navigate to the `SageMaker/advanced-rag-amazon-opensearch/retrieval-augment-generation` directory by using following command. \n",
    "```\n",
    "cd SageMaker/advanced-rag-amazon-opensearch/retrieval-augment-generation\n",
    "```\n",
    "\n",
    "6. Unzip the uploaded zip file using following command\n",
    "\n",
    "```\n",
    "unzip archive.zip\n",
    "```\n",
    "\n",
    "Make sure the unzipped file `winmag-data-130k-v2.json` is in the same directory as this python notebook.\n",
    "\n",
    "After downloading and extracting the json file, execute the following cells to inspect the dataset, transform it into a pandas DataFrame, and sample a subset of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d48f448",
   "metadata": {},
   "source": [
    "#### Sampling subset of the records to load into opensearch quickly\n",
    "Since the data is composed of 129,000 records, it could take some time to convert them into vectors and load them in a vector store. Therefore, we will take a subset (300 records) of our data. We will add a variable called record_id which corresponds to the index of the record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a462a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Following code will not work without completing the above steps \n",
    "df = pd.read_json('winemag-data-130k-v2.json')\n",
    "df_sample = df.sample(300,random_state=37).reset_index()\n",
    "df_sample['record_id'] = range(1, len(df_sample) + 1)\n",
    "df_sample[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f54a349",
   "metadata": {},
   "source": [
    "## 3. Create a connection with OpenSearch cluster.\n",
    "Next, we'll use Python API to set up connection with OpenSearch Cluster.\n",
    "\n",
    "#### Important pre-requisite\n",
    "You should have followed the steps in the Lab instruction section to map Sagemaker notebook role to OpenSearch `ml_full_access` role. If not, please visit the lab instructions and complete the **Setting up permission for Notebook IAM Role** section.\n",
    "\n",
    "#### Retrieving credentials from Secrets manager\n",
    "We are going to use Sagemaker Notebook IAM role to configure the workflows in OpenSearch. This IAM Role has permission to pass BedrockInference IAM role to OpenSearch. OpenSearch will then be able to use BedrockInference IAM role to make calls to Bedrock models.\n",
    "\n",
    "##### NOTE: \n",
    "_At any point in this exercise if you get a failure message - **The security token included in the request is expired.**_ You can resolve it by running this cell. The cell refreshes the security credentials we will be using through the rest of the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405e0e52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kms = boto3.client('secretsmanager')\n",
    "aos_credentials = json.loads(kms.get_secret_value(SecretId=outputs['OpenSearchSecret'])['SecretString'])\n",
    "\n",
    "#credentials = boto3.Session().get_credentials()\n",
    "#auth = AWSV4SignerAuth(credentials, region)\n",
    "auth = (aos_credentials['username'], aos_credentials['password'])\n",
    "\n",
    "aos_client = OpenSearch(\n",
    "    hosts = [{'host': aos_host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")\n",
    "ml_client = MLCommonClient(aos_client)\n",
    "\n",
    "#initializing some variables that we will use later.\n",
    "\n",
    "connector_id = \"\"\n",
    "model_id = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2abe0d8",
   "metadata": {},
   "source": [
    "## 4. Create and deploy model connector to Amazon Bedrock titan v2 \n",
    "\n",
    "Following cell will create a connector using Notebook role. Following cell will create a connection with Amazon Bedrock titan v2 model. Following cell defines the connector configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8c46c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import requests \n",
    "from requests_aws4auth import AWS4Auth\n",
    "import json\n",
    "\n",
    "\n",
    "if not connector_id:\n",
    "    host = f'https://{aos_host}/'\n",
    "    service = 'es'\n",
    "    credentials = boto3.Session().get_credentials()\n",
    "    awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\n",
    "\n",
    "\n",
    "    # Register repository\n",
    "    path = '_plugins/_ml/connectors/_create'\n",
    "    url = host + path\n",
    "\n",
    "    payload = {\n",
    "        \"name\": \"Amazon Bedrock Connector: embedding\",\n",
    "        \"description\": \"The connector to bedrock Titan embedding model\",\n",
    "        \"version\": 1,\n",
    "        \"protocol\": \"aws_sigv4\",\n",
    "        \"credential\": {\n",
    "          \"roleArn\": f\"arn:aws:iam::{account_id}:role/{bedrock_inf_iam_role}\"\n",
    "       },\n",
    "       \"parameters\": {\n",
    "        \"region\": region,\n",
    "        \"service_name\": \"bedrock\",\n",
    "        \"model\": \"amazon.titan-embed-text-v2:0\"\n",
    "       },\n",
    "       \"actions\": [\n",
    "        {\n",
    "          \"action_type\": \"predict\",\n",
    "          \"method\": \"POST\",\n",
    "          \"url\": \"https://bedrock-runtime.${parameters.region}.amazonaws.com/model/${parameters.model}/invoke\",\n",
    "          \"headers\": {\n",
    "            \"content-type\": \"application/json\",\n",
    "            \"x-amz-content-sha256\": \"required\"\n",
    "          },\n",
    "         \"request_body\": \"{ \\\"inputText\\\": \\\"${parameters.inputText}\\\" }\",\n",
    "         \"pre_process_function\": \"connector.pre_process.bedrock.embedding\",\n",
    "         \"post_process_function\": \"connector.post_process.bedrock.embedding\"}\n",
    "       ]\n",
    "    }\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "    print(r.status_code)\n",
    "    print(r.text)\n",
    "    connector_id = json.loads(r.text)[\"connector_id\"]\n",
    "else:\n",
    "    print(f\"Connector already exists - {connector_id}\")\n",
    "    \n",
    "connector_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b508db",
   "metadata": {},
   "source": [
    "Once the model connector is defined. We need to register the model and deploy. Following two cells will register and then deploy the model connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c097af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the model\n",
    "if not model_id:\n",
    "    path = '_plugins/_ml/models/_register'\n",
    "    url = 'https://'+aos_host + '/' + path\n",
    "    payload = { \"name\": \"Bedrock Titan embeddings model\",\n",
    "    \"function_name\": \"remote\",\n",
    "    \"description\": \"Bedrock Titan text embeddings model\",\n",
    "    \"connector_id\": connector_id}\n",
    "    r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "    model_id = json.loads(r.text)[\"model_id\"]\n",
    "else:\n",
    "    print(\"skipping model registration - model already exists\")\n",
    "print(\"Model registered under model_id: \"+model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070ecda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model\n",
    "path = '_plugins/_ml/models/'+model_id+'/_deploy'\n",
    "url = 'https://'+aos_host + '/' + path\n",
    "r = requests.post(url, auth=awsauth, headers=headers)\n",
    "deploy_status = json.loads(r.text)[\"status\"]\n",
    "print(\"Deployment status of the model, \"+model_id+\" : \"+deploy_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac09e6d",
   "metadata": {},
   "source": [
    "#### Create a test embedding to see model deploying working alright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3c8d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences = [\"food for my wine\"]\n",
    "embedding_output = ml_client.generate_embedding(f\"{model_id}\", input_sentences)\n",
    "embed = embedding_output['inference_results'][0]['output'][0]['data']\n",
    "print(embed[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13af09f4",
   "metadata": {},
   "source": [
    "## 5. Create ingest pipeline\n",
    "Let's create an ingestion pipeline that will call Amazon Bedrock titan model and convert the description field in the wine review to vector embedding. Ingest pipeline is a concept in OpenSearch that allows you to define certain actions to be performed at the time of data ingestion. You could do simple processing such as adding a static field, modify an existing field in the data, or call a remote model to get inference and store inference output together with the indexed record/document.\n",
    "\n",
    "Following ingestion pipeline is going to call our remote model and convert wine review `description` field to vector and store it in the field called `description_embedding`\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e6ebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  \"/_ingest/pipeline/wine-ingest-pipeline\"\n",
    "url = f\"{aos_host}{path}\"\n",
    "\n",
    "payload = {\n",
    "  \"description\": \"An Wine index ingest pipeline\",\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"text_embedding\": {\n",
    "        \"model_id\": f\"{model_id}\",\n",
    "        \"field_map\": {\n",
    "          \"description\": \"description_embedding\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "aos_client.ingest.put_pipeline(id=\"wine-ingest-pipeline\", body=payload)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaabc1e",
   "metadata": {},
   "source": [
    "## 6. Create a index in Amazon Opensearch Service \n",
    "Now we will define our index for wine reviews. We are going to define multiple fields from original data and a KNN field that will store description embeddings.\n",
    "\n",
    "To create the index, we first define the index in JSON, then use the aos_client connection we initiated ealier to create the index in OpenSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba5754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "knn_index = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True,\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"default_pipeline\": \"wine-ingest-pipeline\", \n",
    "        \"analysis\": {\n",
    "          \"analyzer\": {\n",
    "            \"default\": {\n",
    "              \"type\": \"standard\",\n",
    "              \"stopwords\": \"_english_\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"description_embedding\": {\n",
    "                    \"type\": \"knn_vector\",\n",
    "                    \"dimension\": 1024,\n",
    "                    \"method\": {\n",
    "                        \"name\": \"hnsw\",\n",
    "                        \"space_type\": \"innerproduct\",\n",
    "                        \"engine\": \"faiss\",\n",
    "                        \"parameters\": {\n",
    "                          \"ef_construction\": 256,\n",
    "                          \"m\": 48\n",
    "                        }\n",
    "                      }\n",
    "                    },\n",
    "            \"description\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"designation\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"variety\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"country\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"winery\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"points\": {\n",
    "                \"type\": \"integer\",\n",
    "                \"store\": True\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e84514d",
   "metadata": {},
   "source": [
    "Using the above index definition, we now need to create the index in Amazon OpenSearch. Running this cell will recreate the index if you have already executed this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464b0ac2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_name = \"wine_knowledge_base\"\n",
    "\n",
    "try:\n",
    "    aos_client.indices.delete(index=index_name)\n",
    "    print(\"Recreating index '\" + index_name + \"' on cluster.\")\n",
    "    aos_client.indices.create(index=index_name,body=knn_index,ignore=400)\n",
    "except:\n",
    "    print(\"Index '\" + index_name + \"' not found. Creating index on cluster.\")\n",
    "    aos_client.indices.create(index=index_name,body=knn_index,ignore=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7007735",
   "metadata": {},
   "source": [
    "Let's verify the created index information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f71659d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aos_client.indices.get(index=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0040992c",
   "metadata": {},
   "source": [
    "## 7. Load the raw data into the Index\n",
    "Next, let's load the wine review data and embedding into the index we've just created. Notice that we will store our embedding in `description_vector` field which will later be used for KNN search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fdd8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "batch = 0\n",
    "action = json.dumps({ \"index\": { \"_index\": index_name } })\n",
    "body_ = ''\n",
    "\n",
    "\n",
    "with alive_bar(len(df_sample), force_tty = True) as bar:\n",
    "    for index, record in (df_sample.iterrows()):\n",
    "\n",
    "        payload={\n",
    "           \"description\": record[\"description\"],\n",
    "           \"points\":record[\"points\"],\n",
    "           \"variety\":record[\"variety\"],\n",
    "           \"country\":record[\"country\"],\n",
    "           \"designation\":record[\"designation\"],\n",
    "           \"winery\":record[\"winery\"]\n",
    "        }\n",
    "        body_ = body_ + action + \"\\n\" + json.dumps(payload) + \"\\n\"\n",
    "        cnt = cnt+1\n",
    "        \n",
    "        if(cnt == 100):\n",
    "            \n",
    "            response = aos_client.bulk(\n",
    "                                index = index_name,\n",
    "                                 body = body_)\n",
    "            cnt = 0\n",
    "            batch = batch +1\n",
    "            body_ = ''\n",
    "        \n",
    "        bar()\n",
    "print(\"Total Bulk batches completed: \"+str(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fad674",
   "metadata": {},
   "source": [
    "To validate the load, we'll query the number of documents number in the index. We should have 300 hits in the index, or however many was specified earlier in sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed0b71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = aos_client.search(index=index_name, body={\"query\": {\"match_all\": {}}})\n",
    "print(\"Records found: %d.\" % res['hits']['total']['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9b827c",
   "metadata": {},
   "source": [
    "## 8. Search vector with \"Semantic Search\" \n",
    "\n",
    "Now we can define a helper function to execute the search query for us to find a wine whose review most closely matches the requested description. `retrieve_opensearch_with_semantic_search` embeds the search phrase, searches the index for the closest matching vector, and returns the top result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ed4ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_opensearch_with_semantic_search(phrase, n=3):\n",
    "    osquery={\n",
    "        \"_source\": {\n",
    "            \"exclude\": [ \"description_embedding\" ]\n",
    "        },\n",
    "        \n",
    "      \"size\": n,\n",
    "      \"query\": {\n",
    "        \"neural\": {\n",
    "          \"description_embedding\": {\n",
    "            \"query_text\": f\"{phrase}\",\n",
    "            \"model_id\": f\"{model_id}\",\n",
    "            \"k\": 5\n",
    "          }\n",
    "        }\n",
    "      }    \n",
    "    }\n",
    "\n",
    "    res = aos_client.search(index=index_name, \n",
    "                           body=osquery,\n",
    "                           stored_fields=[\"description\",\"winery\",\"points\", \"designation\", \"country\"],\n",
    "                           explain = True)\n",
    "    top_result = res['hits']['hits']\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for entry in top_result:\n",
    "        result = {\n",
    "            \"description\":entry['_source']['description'],\n",
    "            \"winery\":entry['_source']['winery'],\n",
    "            \"points\":entry['_source']['points'],\n",
    "            \"designation\":entry['_source']['designation'],\n",
    "            \"country\":entry['_source']['country'],\n",
    "            \"variety\":entry['_source']['variety'],\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f6820a",
   "metadata": {},
   "source": [
    "Use the semantic search to get similar records with the sample question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d529077",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_on_wine = \"great tasting wine for thanks giving\"\n",
    "example_request = retrieve_opensearch_with_semantic_search(question_on_wine)\n",
    "print(json.dumps(example_request, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98987279",
   "metadata": {},
   "source": [
    "## 9. Prepare a method to call Amazon Bedrock - Anthropic Claude Sonnet model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd47ade",
   "metadata": {},
   "source": [
    "Now we will define a function to call LLM to answer user's question. As LLM is trained with static data, and it does not have our wine review knowledge. While it may be able to answer, it may not be an answer that a business prefers. For example. in our case, we would not want to recommend a wine that we do not stock. So the recommendation has to be one of the wines from our collection i.e. 300 reviews that we loaded. \n",
    "\n",
    "After defining this function we will call it to see how LLM answers questions without the wine review data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd66920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm_endpoint_with_json_payload(encoded_json):\n",
    "\n",
    "    # Create a Bedrock Runtime client\n",
    "    bedrock_client = boto3.client('bedrock-runtime')\n",
    "    # Set the model ID for Claude 3 Sonnet\n",
    "    model_id = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "    accept = 'application/json'\n",
    "    content_type = 'application/json'\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Invoke the model with the native request payload\n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=str.encode(str(encoded_json)),\n",
    "            accept = accept,\n",
    "            contentType=content_type\n",
    "        )\n",
    "\n",
    "        # Decode the response body\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        return response_body\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return none\n",
    "\n",
    "def query_llm(system, user_question):\n",
    "\n",
    "    # Prepare the model's payload\n",
    "    payload = json.dumps({\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 10000,\n",
    "        \"system\": system,\n",
    "        \"messages\": [\n",
    "            {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": [\n",
    "                {\n",
    "                  \"type\": \"text\",\n",
    "                  \"text\": f\"{user_question}\"\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        })\n",
    "    \n",
    "\n",
    "\n",
    "    query_response = query_llm_endpoint_with_json_payload(payload)\n",
    "\n",
    "    return query_response['content'][0]['text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb63ed8",
   "metadata": {},
   "source": [
    "Let's check the generated result for a wine recommendation without retrieving our reviews. It may not be one of the wine that we stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm_without_rag(question):\n",
    "\n",
    "    # In claude system prompt defines the role you want the Claude model to play\n",
    "    system_prompt = \"You are a sommelier that uses their vast knowledge of wine to make great recommendations people will enjoy.\"\n",
    "    \n",
    "    # User prompt contains the instructions for the model.\n",
    "    user_prompt = f\"As a sommelier, you must include the wine variety, the country of origin, and a colorful description relating to the customer question.\\n Customer Question:{question}. Please provide name of the wine at the end of the answer, in a new line, in format Wine name: <wine name>\"\n",
    "    return query_llm(system_prompt, user_prompt)\n",
    "\n",
    "question_on_wine = \"Which Portuguese wine goes well with a steak?\"\n",
    "\n",
    "print(f\"The recommened wine from LLM without RAG: \\n\\n{query_llm_without_rag(question_on_wine)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5191cc2a",
   "metadata": {},
   "source": [
    "#### Testing for hallucination. \n",
    "Let's copy the wine name from the last line and past it in the question variable below to see if we have this wine in our stock. Please review the list of wines that are returned. They may be from portugal but not exactly the one we have been recommended by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702eb67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_name = \"Quinta do Vesuvio Touriga Nacional\"\n",
    "example_request = retrieve_opensearch_with_semantic_search(wine_name)\n",
    "print(json.dumps(example_request, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d9e654",
   "metadata": {},
   "source": [
    "## 10. Retrieval Augmented Generation\n",
    "---\n",
    "To resolve LLM hallunination problem, we can more context to LLM so that LLM can use context information to fine the model and generated factual result. RAG is one of the solution to the LLM hallucination. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fa5564",
   "metadata": {},
   "source": [
    "#### Create a prompt for the LLM using the search results from OpenSearch (RAG)\n",
    "\n",
    "We will be using the Anthropic Sonnet model with one-shot prompting technique. Within instructions to the model in the prompt, we will provide a sample wine review and how model should use to answer user's question. At the end of the prompt wine reviews retrieved from Opensearch will be included for model to use. \n",
    "\n",
    "Before querying the model, the below function `generate_rag_based_system_prompt` is used to put together user prompt. The function takes in an input string to search the OpenSearch cluster for a matching wine, then compose the user prompt for LLM. \n",
    "\n",
    "System prompt defines the role that LLM plays.\n",
    "\n",
    "User prompt contains the instructions and the context information that LLM model uses to answer user's question.\n",
    "\n",
    "The prompt is in the following format:\n",
    "\n",
    "**SYSTEM PROMPT:**\n",
    "\n",
    "```\n",
    "You are a sommelier that uses their vast knowledge of wine to make great recommendations people will enjoy. \n",
    "```\n",
    "\n",
    "\n",
    "**USER PROMPT**\n",
    "```\n",
    "As a sommelier, you must include the wine variety, the country of origin, and a colorful description relating to the user's question.\n",
    "\n",
    "Data:{'description': 'This perfumey white dances in intense and creamy layers of stone fruit and vanilla, remaining vibrant and balanced from start to finish. The generous fruit is grown in the relatively cooler Oak Knoll section of the Napa Valley. This should develop further over time and in the glass.', 'winery': 'Darioush', 'points': 92, 'designation': None, 'country': 'US'}\n",
    "\n",
    "Recommendation:I have a wonderful wine for you. It's a dry, medium bodied white wine from Darioush winery in the Oak Knoll section of Napa Valley, US. It has flavors of vanilla and oak. It scored 92 points in wine spectator.\n",
    "\n",
    "Data: {retrieved_documents}\n",
    "\n",
    "Question from the user as is\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26aa75c",
   "metadata": {},
   "source": [
    "### package the prompt and query the LLM\n",
    "We will create a final function to query the LLM with the prompt. `query_llm_with_rag` is a function that calls LLM in a RAG.\n",
    "\n",
    "`query_llm_with_rag` combines everything we've done in this module. It does all of the following:\n",
    "- searches the OpenSearch index with semantic search for the relevant wine with \"description vector\"\n",
    "- generate an LLM prompt from the search results\n",
    "- queriy the LLM with RAG for a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073b9d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm_with_rag(user_question):\n",
    "    retrieved_documents = retrieve_opensearch_with_semantic_search(user_question)\n",
    "    one_shot_description_example = \"{'description': 'This perfumey white dances in intense and creamy layers of stone fruit and vanilla, remaining vibrant and balanced from start to finish. The generous fruit is grown in the relatively cooler Oak Knoll section of the Napa Valley. This should develop further over time and in the glass.', 'winery': 'Darioush', 'points': 92, 'designation': None, 'country': 'US'}\"\n",
    "    one_shot_response_example = \"I have a wonderful wine for you. It's a dry, medium bodied white wine from Darioush winery in the Oak Knoll section of Napa Valley, US. It has flavors of vanilla and oak. It scored 92 points in wine spectator.\"\n",
    "    system_prompt= \"You are a sommelier that uses vast knowledge of wine to make great recommendations people will enjoy\"\n",
    "    user_prompt = (\n",
    "        f\"As a sommelier, you must include the wine variety, the country of origin, and a colorful description relating to the user question. You are must pick a wine in \\\"Wine data\\\" section only, one that matches best the customer question. Do not suggest anything outside of the wine data provided. You don't necessarily have to pick the top rated wine if its not best suited for customer question.\\n\"\n",
    "        f\"Wine data: {one_shot_description_example} \\n Recommendation: {one_shot_response_example} \\n\"\n",
    "        f\"Wine data: {retrieved_documents} \\n\"\n",
    "        f\"Customer Question: {user_question} \\n\"        \n",
    "    )\n",
    "    response = query_llm(system_prompt, user_prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62de7273",
   "metadata": {},
   "source": [
    "#### And finally, let's call the function and get a wine recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ccf971",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_on_wine=\"Best Australian wine that goes great with beef ?\"\n",
    "recommendation = query_llm_with_rag(question_on_wine)\n",
    "print(recommendation)\n",
    "\n",
    "print(f\"\\n\\ndocuments retrieved for above recommendations were \\n\\n{json.dumps(retrieve_opensearch_with_semantic_search(question_on_wine), indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59343ab",
   "metadata": {},
   "source": [
    "#### Let's change it to Italian wine - it should produce a matching result.\n",
    "We will call the same method again to see if there is an italian wine in our catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e81ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_on_wine=\"Best Italian wine that goes great with steak?\"\n",
    "recommendation = query_llm_with_rag(question_on_wine)\n",
    "print(recommendation)\n",
    "\n",
    "print(f\"\\n\\ndocuments retrieved for above recommendations were \\n\\n{json.dumps(retrieve_opensearch_with_semantic_search(question_on_wine), indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b743e3",
   "metadata": {},
   "source": [
    "You might notice that we asked for Australian wines that goes well with steak and we do not have any such wine in our collection. Therefore the model politely excuses. You may change the question and see how LLM recommends a wine from our select list that best suites your question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2adc9a2",
   "metadata": {},
   "source": [
    "### Additional info: changing kwargs for querying the LLM\n",
    "If you want to change or add new parameters for LLM querying, you're able to add in new keyword arguments to the `query_llm` function. For example, to change the `temperature` value, simply change the function call:\n",
    "`query_llm(description phrase, temperature = new float value)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfff7f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
