{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e87dc259",
   "metadata": {},
   "source": [
    "# RAG using OpenSearch Flow Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cfd51d",
   "metadata": {},
   "source": [
    "\n",
    "OpenSearch provides strong ML capabilities as building blocks to create powerful use cases. Some of the ML features such as Neural semantic search, hybrid search or multi modal search or other ML inference capabilities are usually put together in a complex workflow to retrieve final search results. Flow framework plugin in OpenSearch is designed to accomplish complex workflow create with a single API call. It offers out of the box common use case templates speeds up developing complex search and Generative AI workflows simple. \n",
    "\n",
    "For e.g. there is a use case called conversational search that allows user to ask question from their knowledge base much like what we built in previous labs, however, instead going through multiple steps of registering multiple models for embedding and generation, we simply use one API call and provide initialisation parameter that deploys an embedding connector to create vector, creates an index that will be used as knowledgebase, configures ingest pipeline and finally registers an LLM model to be used to answer user's conversational search questions. All with single API call.\n",
    "\n",
    "\n",
    "Before we explain the API call, we will need to install some of the same libraries to be able to run through the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571c4d1e",
   "metadata": {},
   "source": [
    "## 1. Lab Pre-requisites\n",
    "\n",
    "#### a. Download and install python dependencies\n",
    "\n",
    "For this notebook we require the use of a few libraries. We'll use the Python clients for OpenSearch and SageMaker, and Python frameworks for text embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66e0320",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opensearch-py-ml accelerate tqdm --quiet\n",
    "!pip install sagemaker --upgrade --quiet\n",
    "!pip install requests_aws4auth --quiet\n",
    "!pip install alive-progress --quiet\n",
    "!pip install deprecated --quiet\n",
    "\n",
    "#OpenSearch Python SDK\n",
    "!pip install opensearch_py  --quiet\n",
    "#Progress bar for for loop\n",
    "!pip install alive-progress  --quiet\n",
    "\n",
    "# As in the previous modules, let's import PyTorch and confirm that the latest version of PyTorch is running. \n",
    "# The version should already be 1.13.1 or higher. If not, we will restart the kernel.\n",
    "\n",
    "import torch\n",
    "pytorch_version = torch.__version__\n",
    "print( f\"Pytorch version: {pytorch_version}\")\n",
    "\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "    \n",
    "if pytorch_version.startswith('1.1'):\n",
    "    from IPython.display import display_html\n",
    "    restartkernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa614bc",
   "metadata": {},
   "source": [
    "#### b. Import libraries & initialize resource information\n",
    "The line below will import all the relevant libraries and modules used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1688f4e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sagemaker\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from sagemaker import get_execution_role\n",
    "import random \n",
    "import string\n",
    "import s3fs\n",
    "from urllib.parse import urlparse\n",
    "from IPython.display import display, HTML\n",
    "from alive_progress import alive_bar\n",
    "from opensearch_py_ml.ml_commons import MLCommonClient\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import requests "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3459fff5",
   "metadata": {},
   "source": [
    "#### c. Get CloudFormation stack output variables\n",
    "\n",
    "We have preconfigured a few resources by creating a cloudformation stack in the account. Information of these resources will be used within this lab. We are going to load some of the information variables here.\n",
    "\n",
    "You can ignore any \"PythonDeprecationWarning\" warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc45a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Boto3 session\n",
    "session = boto3.Session()\n",
    "\n",
    "# Get the account id\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# Get the current region\n",
    "region = session.region_name\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "# Method to obtain output variables from Cloudformation stack. \n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "## Setup variables to use for the rest of the demo\n",
    "cloudformation_stack_name = \"advanced-opensearch-rag\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "aos_host = outputs['OpenSearchDomainEndpoint']\n",
    "s3_bucket = outputs['s3BucketTraining']\n",
    "bedrock_inf_iam_role = outputs['BedrockBatchInferenceRole']\n",
    "bedrock_inf_iam_role_arn = outputs['BedrockBatchInferenceRoleArn']\n",
    "sagemaker_notebook_url = outputs['SageMakerNotebookURL']\n",
    "\n",
    "# We will just print all the variables so you can easily copy if needed.\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6607721",
   "metadata": {},
   "source": [
    "## 2. Prepare data\n",
    "_**Note:** This lab uses the same data as earlier labs. If you have downloaded the data in previous lab, you can skip the cell below._\n",
    "\n",
    "Semantic search with a generative model to present the retrieved data to the user . Below is a dataset of wine reviews, we'll sample this data set to recommend wines that resemble the user provided description.\n",
    "\n",
    "#### Mandatory steps to download the data manually\n",
    "Within these labs you will need to download the dataset from various sources. One is Kaggle (You will need to create a free account):\n",
    "https://www.kaggle.com/datasets/christopheiv/winemagdata130k?select=winemag-data-130k-v2.json\n",
    "\n",
    "Visit the above link and click **Download** button. Once downloaded in your laptop, you will resume with following steps.\n",
    "\n",
    "1. Execute the following cell to get URL to SageMaker Notebook. Click the URL to open sagemaker notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9fc342",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = f'<a href=\"{sagemaker_notebook_url}\" target=\"_blank\">Sagemaker notebook URL</a>'\n",
    "display(HTML(link))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a216887",
   "metadata": {},
   "source": [
    "2. Browse to the `retrieval-augment-generation` directory\n",
    "3. Click \"Upload\" to upload the zip downloaded from Kaggle\n",
    "4. Click \"New\" -> \"Terminal\" to open a terminal window\n",
    "5. Navigate to the `SageMaker/advanced-rag-amazon-opensearch/retrieval-augment-generation` directory by using following command. \n",
    "```\n",
    "cd SageMaker/advanced-rag-amazon-opensearch/retrieval-augment-generation\n",
    "```\n",
    "\n",
    "6. Unzip the uploaded zip file using following command\n",
    "\n",
    "```\n",
    "unzip archive.zip\n",
    "```\n",
    "\n",
    "Make sure the unzipped file `winmag-data-130k-v2.json` is in the same directory as this python notebook.\n",
    "\n",
    "After downloading and extracting the json file, execute the following cells to inspect the dataset, transform it into a pandas DataFrame, and sample a subset of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d48f448",
   "metadata": {},
   "source": [
    "#### Sampling subset of the records to load into opensearch quickly\n",
    "Since the data is composed of 129,000 records, it could take some time to convert them into vectors and load them in a vector store. Therefore, we will take a subset (300 records) of our data. We will add a variable called record_id which corresponds to the index of the record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a462a",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Following code will not work without completing the above steps \n",
    "df = pd.read_json('winemag-data-130k-v2.json')\n",
    "df_sample = df.sample(300,random_state=37).reset_index()\n",
    "df_sample['record_id'] = range(1, len(df_sample) + 1)\n",
    "df_sample[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f54a349",
   "metadata": {},
   "source": [
    "## 3. Create a connection with OpenSearch cluster.\n",
    "Next, we'll use Python API to set up connection with OpenSearch Cluster.\n",
    "\n",
    "#### Important pre-requisite\n",
    "You should have followed the steps in the Lab instruction section to map Sagemaker notebook role to OpenSearch `ml_full_access` and `flow_framework_full_access` roles. If not, please visit the lab instructions and complete the **Setting up permission for Notebook IAM Role** section.\n",
    "\n",
    "#### Retrieving credentials from Secrets manager\n",
    "We are going to use Sagemaker Notebook IAM role to configure the workflows in OpenSearch. This IAM Role has permission to pass BedrockInference IAM role to OpenSearch. OpenSearch will then be able to use BedrockInference IAM role to make calls to Bedrock models.\n",
    "\n",
    "##### NOTE: \n",
    "_At any point in this exercise if you get a failure message - **The security token included in the request is expired.**_ You can resolve it by running this cell. The cell refreshes the security credentials we will be using through the rest of the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405e0e52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kms = boto3.client('secretsmanager')\n",
    "aos_credentials = json.loads(kms.get_secret_value(SecretId=outputs['OpenSearchSecret'])['SecretString'])\n",
    "\n",
    "service = 'es'\n",
    "\n",
    "# Retrieves credential from current NoteBook session which is running as NBRole IAM role.\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\n",
    "\n",
    "aos_client = OpenSearch(\n",
    "    hosts = [{'host': aos_host, 'port': 443}],\n",
    "    http_auth = awsauth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")\n",
    "ml_client = MLCommonClient(aos_client)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5488c484",
   "metadata": {},
   "source": [
    "## 4. Single API call titan text embedding connector deployment\n",
    "\n",
    "Amazon OpenSearch service has launched flow framework starting from v2.13. This version offers few out of the box workflow templates. For more information read [here](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/ml-workflow-framework.html).\n",
    "\n",
    "We will be using one of the out of the box templates to deploy Amazon Bedrock connector with single API call. The template is called - `bedrock_titan_embedding_model_deploy`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4761579c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create path to the Workflow provisioning API\n",
    "use_case_template_name = 'bedrock_titan_embedding_model_deploy'\n",
    "path = f'_plugins/_flow_framework/workflow?use_case={use_case_template_name}&provision=true'\n",
    "url = f'https://{aos_host}/{path}'\n",
    "titan_v2_embedding_model_id = \"amazon.titan-embed-text-v2:0\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "print(url)\n",
    "\n",
    "# Initializing the flow workflow template parameters that we referenced in the template at the time of \n",
    "# creating the template\n",
    "\n",
    "default_params = {\n",
    "    \"template.name\": \"bedrock_titan_embedding_model_deploy\",\n",
    "    \"create_connector.region\": f\"{region}\",\n",
    "    \"create_connector.credential.roleArn\": f\"{bedrock_inf_iam_role_arn}\",\n",
    "    \"create_connector.actions.url\": f\"https://bedrock-runtime.{region}.amazonaws.com/model/{titan_v2_embedding_model_id}/invoke\",\n",
    "}\n",
    "\n",
    "# Calling API for provisioning workflow.\n",
    "r = requests.post(url, auth=awsauth, json=default_params, headers=headers)\n",
    "\n",
    "#print status of the API call.\n",
    "print(f\"Status: {r.status_code}. Response:{r.text}\")\n",
    "\n",
    "\n",
    "# if status is success then obtain the workflow id for provisioning in the next step\n",
    "if r.status_code == 200 or r.status_code == 201 :\n",
    "    print(r.text)\n",
    "    workflow_id = json.loads(r.text)[\"workflow_id\"]\n",
    "else: \n",
    "    print(f\"failed {r.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c2865a",
   "metadata": {},
   "source": [
    "#### Check the status of provisioning\n",
    "Following cell returns the status of the provisioning request and also captures the embedding model id in a variable which we can use to genereate the text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a867e918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put together URL to _status API of flow framework\n",
    "path = f'_plugins/_flow_framework/workflow/{workflow_id}/_status'\n",
    "url = f'https://{aos_host}/{path}'\n",
    "print(url)\n",
    "\n",
    "\n",
    "# We will capture various model ids so that we can test them individually if we wanted to.\n",
    "embedding_model_id=\"\"\n",
    "\n",
    "# Calling the _status API\n",
    "r = requests.get(url, auth=awsauth, json=default_params, headers=headers)\n",
    "\n",
    "print(f\"Status: {r.status_code}.\")\n",
    "\n",
    "# if _status API call returns successfully, we should extract the data we need.\n",
    "if r.status_code == 200: \n",
    "    status = json.loads(r.text)[\"state\"]\n",
    "    if status == \"COMPLETED\":\n",
    "        response_json = json.loads(r.text)\n",
    "        print(json.dumps( response_json , indent=4))\n",
    "        for resources in response_json[\"resources_created\"]:\n",
    "            if resources[\"workflow_step_id\"] == \"register_model\":\n",
    "                embedding_model_id=resources[\"resource_id\"]    \n",
    "        print(f\"Embedding model successfully deployed. ID: {embedding_model_id}\")    \n",
    "    else: \n",
    "        print(f\"Failed or Pending Response. Please see the details below \\n{r.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd98b137",
   "metadata": {},
   "source": [
    "#### Let's test the deployed titan text model connector\n",
    "We will generate text embedding and print first 5 dimensions out of 1024 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798fdaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can test the embedding generation here.\n",
    "payload = {\n",
    "    \"parameters\":\n",
    "    { \n",
    "        \"inputText\": \"Best wine that goes with beef\"\n",
    "    } \n",
    "}\n",
    "\n",
    "path = '_plugins/_ml/models/'+embedding_model_id+'/_predict'\n",
    "url = 'https://'+aos_host + '/' + path\n",
    "r = requests.post(url, auth=awsauth, json=payload , headers=headers )\n",
    "\n",
    "if r.status_code == 200:\n",
    "    embedding = json.loads(r.text)['inference_results'][0]['output'][0]['data']\n",
    "    print(embedding[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f6062a",
   "metadata": {},
   "source": [
    "### What did we do?\n",
    "We deployed a titan embedding model with just one API call by providing a template name `bedrock_titan_embedding_model_deploy` and supplying only the parameters we would like to override. In our case we overrode the default G1 model deployment with latest (to date: July 2024) titan v2 model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733b51c6",
   "metadata": {},
   "source": [
    "## 5. Flow framework workflows\n",
    "While above was a very minimal example of a single API call deployment. Following is a more elaborate example of RAG architecture created as a workflow. OpenSearch has capability to call Amazon Bedrock service for not only embedding generation but also text generation. We can create a complete RAG architecture using flow custom templates. This custom template will achieve following.\n",
    "\n",
    "1. Registers and deploys a connection to Anthropic Claude 3 Sonnet model throught Amazon Bedrock service.\n",
    "2. Registers and deploys a connection to Amazon Bedrock Titan text model for text to vector embedding generation.\n",
    "3. Creates an ingestion pipeline that uses above Bedrock titan model to convert text to vector and put it in `description_embedding` variable.\n",
    "4. Creates an index and sets the above ingestion pipeline as its default ingestion pipeline.\n",
    "5. Creates a [RAG Tool](https://opensearch.org/docs/latest/ml-commons-plugin/agents-tools/tools/rag-tool/) that is a OpenSearch ML common plugin feature. It helps create RAG architecture. It takes as input user's question, it runs a semantic search, and passes the output of the semantic search result to LLM like Anthropic claude to generate answer. Much like what we did with our code in previous labs.\n",
    "6. Last step, the workflow creates a Root agent that launches the above RAG tool to answer user's question.\n",
    "\n",
    "First we will define a workflow and in the second step we will provision the workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c597ae",
   "metadata": {},
   "source": [
    "### Define a custom workflow\n",
    "We are going to define a complete RAG workflow. Each node in the workflow represents steps taken to achieve the above defined sequence.\n",
    "\n",
    "The RAG we are creating is exactly the same as we did in the previous Lab. It is a Wine sommelier bot that helps answer user's questions about wine from a select list of wines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a669aba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a path to flow framework API\n",
    "path = '_plugins/_flow_framework/workflow?'\n",
    "url = f'https://{aos_host}/{path}'\n",
    "\n",
    "# Most API calls we will use now works using application/json content type.\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "\n",
    "#initialise variables to hold connector and model id.\n",
    "connector_id = \"\"\n",
    "model_id = \"\"\n",
    "workflow_id = \"\"\n",
    "\n",
    "payload = {\n",
    "    \"name\": \"deploy-bedrock-chat-model\",\n",
    "    \"description\": \"Deploys a Wine sommelier RAG chatbot\",\n",
    "    \"use_case\": \"my_rag_chat\",\n",
    "    \"version\": {\n",
    "        \"template\": \"1.0.0\",\n",
    "        \"compatibility\": [\n",
    "            \"2.12.0\",\n",
    "            \"3.0.0\"\n",
    "        ]\n",
    "    },\n",
    "    \"workflows\": {\n",
    "        \"provision\": {\n",
    "            \"nodes\": [\n",
    "                     {\n",
    "                      #this node creates a Bedrock connection with Anthropic Claude Sonnet 3 model.\n",
    "                      \"id\": \"create_bedrock_connector\",\n",
    "                      \"type\": \"create_connector\",\n",
    "                      \"user_inputs\": {                          \n",
    "                        \"name\": \"Amazon Bedrock Connector: Claude Sonnet 3\",\n",
    "                        \"version\": \"1\",\n",
    "                        \"protocol\": \"aws_sigv4\",\n",
    "                        \"description\": \"The connector to bedrock Claude Sonnet model\",\n",
    "                        \"actions\": [\n",
    "                          {\n",
    "                            \"headers\": {\n",
    "                              \"content-type\": \"application/json\"\n",
    "                            },\n",
    "                            \"method\": \"POST\",\n",
    "                            \"request_body\": \"{\\\"system\\\": \\\"You are a sommelier that uses their vast knowledge of wine to make great recommendations people will enjoy.\\\", \\\"messages\\\":[{\\\"role\\\": \\\"user\\\", \\\"content\\\":[{\\\"type\\\":\\\"text\\\", \\\"text\\\":\\\"${parameters.prompt}. Customer question: ${parameters.question}\\\"}]}], \\\"max_tokens\\\":${parameters.max_tokens_to_sample}, \\\"temperature\\\":${parameters.temperature},  \\\"anthropic_version\\\":\\\"${parameters.anthropic_version}\\\" }\",                    \n",
    "                            \"action_type\": \"predict\",\n",
    "                            \"url\": f\"https://bedrock-runtime.{region}.amazonaws.com/model/anthropic.claude-3-sonnet-20240229-v1:0/invoke\",\n",
    "                        }\n",
    "                        ],\n",
    "                        \"credential\": {\n",
    "                            \"roleArn\": f\"{bedrock_inf_iam_role_arn}\" \n",
    "                         },\n",
    "                        \"parameters\": {\n",
    "                          \"endpoint\": \"bedrock-runtime.us-east-1.amazonaws.com\",\n",
    "                          \"content_type\": \"application/json\",\n",
    "                          \"auth\": \"Sig_V4\",\n",
    "                          \"max_tokens_to_sample\": \"8000\",\n",
    "                          \"service_name\": \"bedrock\",\n",
    "                          \"temperature\": \"0.0001\",\n",
    "                          \"response_filter\": \"$.content[0].text\",\n",
    "                          \"region\": f\"{region}\",\n",
    "                          \"anthropic_version\": \"bedrock-2023-05-31\"\n",
    "                        }\n",
    "                      }\n",
    "                },\n",
    "                {\n",
    "                    #this node creates Amazon Bedrock titan v2 connector\n",
    "                    \"id\": \"create_embedding_connector\",\n",
    "                    \"type\": \"create_connector\",\n",
    "                    \"user_inputs\": {\n",
    "                        \"name\": \"${{create_embedding_connector.name}}\",\n",
    "                        \"description\": \"${{create_embedding_connector.description}}\",\n",
    "                        \"version\": \"1\",\n",
    "                        \"protocol\": \"aws_sigv4\",\n",
    "                        \"credential\": {\n",
    "                            \"roleArn\" : f\"{bedrock_inf_iam_role_arn}\"\n",
    "                        },\n",
    "                        \"parameters\": {\n",
    "                            \"service_name\": \"bedrock\",\n",
    "                            \"model\": \"amazon.titan-embed-text-v2:0\",\n",
    "                            \"region\": f\"{region}\",\n",
    "                            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                        },\n",
    "                        \"actions\": [\n",
    "                              {\n",
    "                                \"action_type\": \"PREDICT\",\n",
    "                                \"method\": \"POST\",\n",
    "                                \"url\": \"https://bedrock-runtime.us-east-1.amazonaws.com/model/amazon.titan-embed-text-v2:0/invoke\",\n",
    "                                \"headers\": {\n",
    "                                  \"content-type\": \"application/json\"\n",
    "                                },\n",
    "                                \"request_body\": \"{ \\\"inputText\\\": \\\"${parameters.inputText}\\\" }\",\n",
    "                                #these are built-in function in opensearch to pre/post process titan embeddings\n",
    "                                \"pre_process_function\": \"connector.pre_process.bedrock.embedding\",\n",
    "                                \"post_process_function\": \"connector.post_process.bedrock.embedding\"\n",
    "                              }\n",
    "                        ]\n",
    "                    }\n",
    "                },\n",
    "\n",
    "                {\n",
    "                    #this node registers Anthropic Claude Sonnet 3 model .                    \n",
    "                    \"id\": \"register_bedrock_model\",\n",
    "                    \"type\": \"register_remote_model\",\n",
    "                    \n",
    "                    \"previous_node_inputs\": {\n",
    "                        \"create_bedrock_connector\": \"connector_id\"\n",
    "                    },\n",
    "                    \"user_inputs\": {\n",
    "                        \"name\": \"anthropic.claude-v3\",\n",
    "                        \"function_name\": \"remote\",\n",
    "                        \"description\": \"bedrock-chat-model\",\n",
    "                        \"deploy\": True\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    #this node registers Amazon Bedrock titan v2 model .                    \n",
    "                    \"id\": \"register_bedrock_embedding_model\",\n",
    "                    \"type\": \"register_remote_model\",\n",
    "                    \"previous_node_inputs\": {\n",
    "                        \"create_embedding_connector\": \"connector_id\"\n",
    "                    },\n",
    "                    \"user_inputs\": {\n",
    "                        \"name\": \"Bedrock embedding model v2\",\n",
    "                        \"description\": \"Bedrock embedding model v2\",\n",
    "                        \"function_name\": \"remote\",\n",
    "                        \"deploy\": True\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    # this node creates ingest pipeline that launches bedrock embedding model\n",
    "                    # for generating embedding before writing data to opensearch\n",
    "                    \n",
    "                    \"id\": \"create_ingest_pipeline\",\n",
    "                    \"type\": \"create_ingest_pipeline\",\n",
    "                    \"previous_node_inputs\": {\n",
    "                        \"register_bedrock_embedding_model\": \"model_id\"\n",
    "                    },\n",
    "                    \"user_inputs\": {\n",
    "                        \"pipeline_id\": \"${{create_ingest_pipeline.pipeline_id}}\",\n",
    "                        \"configurations\": {\n",
    "                            \"description\": \"A neural ingest pipeline\",\n",
    "                            \"processors\": [\n",
    "                                {\n",
    "                                    \"text_embedding\": {\n",
    "                                      \"model_id\": \"${{register_bedrock_embedding_model.model_id}}\",\n",
    "                                      \"field_map\": {\n",
    "                                        \"${{text_embedding.field_map.input}}\": \"${{text_embedding.field_map.output}}\"\n",
    "                                      }\n",
    "                                    }\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    # This node would create an index to capture wine reviews and embeddings\n",
    "                    \"id\": \"create_index\",\n",
    "                    \"type\": \"create_index\",\n",
    "                    \"previous_node_inputs\": {\n",
    "                        \"create_ingest_pipeline\": \"pipeline_id\"\n",
    "                    },\n",
    "                    \"user_inputs\": {\n",
    "                        \"index_name\": \"wine_knowledge_base2\",\n",
    "                        \"configurations\": {\n",
    "                            \"settings\": {\n",
    "                                \"index\": {\n",
    "                                    \"default_pipeline\": \"${{create_ingest_pipeline.pipeline_id}}\",\n",
    "                                    \"knn\": \"true\"\n",
    "                                }\n",
    "                            },\n",
    "                            \"mappings\": {\n",
    "                                \"properties\": {\n",
    "                                  \"${{text_embedding.field_map.input}}\": {\n",
    "                                    \"type\": \"text\"\n",
    "                                  },\n",
    "                                  \"${{text_embedding.field_map.output}}\": {\n",
    "                                    \"type\": \"knn_vector\",\n",
    "                                    \"method\": {\n",
    "                                        \"engine\": \"lucene\",\n",
    "                                        \"space_type\": \"l2\",\n",
    "                                        \"name\": \"hnsw\",\n",
    "                                        \"parameters\": {}\n",
    "                                    },\n",
    "                                    \"dimension\": \"1024\"\n",
    "                                  }\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    # this node would deploy a RAG tool that will launch embedding model first to \n",
    "                    # generate text embedding for user's question. Then it will search opensearch\n",
    "                    # for semantically similar reviews, and then passes on the returned results with\n",
    "                    # a prompt to Anthropic claude 3 model to generate an answer.\n",
    "                    \"id\": \"rag_tool\",\n",
    "                    \"type\": \"create_tool\",\n",
    "                     \"previous_node_inputs\": {\n",
    "                        \"register_bedrock_model\": \"model_id\",\n",
    "                        \"register_bedrock_embedding_model\": \"model_id\"\n",
    "                    },\n",
    "                    \"user_inputs\": {\n",
    "                        \"type\": \"RAGTool\",\n",
    "                        \"name\": \"RAGTool\",\n",
    "                        \"parameters\": {\n",
    "                            \"inference_model_id\": \"${{register_bedrock_model.model_id}}\",\n",
    "                            \"embedding_model_id\": \"${{register_bedrock_embedding_model.model_id}}\",\n",
    "                            \"index\": \"${{create_index.name}}\",\n",
    "                            \"embedding_field\": \"${{text_embedding.field_map.output}}\",\n",
    "                            \"source_field\": \"[\\\"${{text_embedding.field_map.input}}\\\",\\\"winery\\\", \\\"points\\\",\\\"designation\\\",\\\"country\\\"]\",\n",
    "                            \"query_type\": \"neural\",\n",
    "                            \"input\": \"${parameters.question}\",\n",
    "                            \"prompt\": \"${{rag_tool.parameters.prompt}}\",\n",
    "                            \"include_output_in_agent_response\": True\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    # to launch RAG tool we need an Agent. This node creates the agent\n",
    "                    # that is configured with RAG Tool as the tool to execute.\n",
    "                    \"id\": \"root_agent\",\n",
    "                    \"type\": \"register_agent\",\n",
    "                    \"previous_node_inputs\": {\n",
    "                        \"rag_tool\": \"tools\"\n",
    "                    },\n",
    "                    \"user_inputs\": {\n",
    "                        \"parameters\": {\n",
    "                            \"prompt\": \"${{root_agent.parameters.parameters}}\"\n",
    "                        },\n",
    "                        \"app_type\": \"chatbot\",\n",
    "                        \"name\": \"Root agent\",\n",
    "                        \"description\": \"this is the root agent\",\n",
    "                        \"tools_order\": [\n",
    "                            \"rag_tool\"\n",
    "                        ],\n",
    "                        \"memory\": {\n",
    "                            \"type\": \"conversation_index\"\n",
    "                        },\n",
    "                        \"type\": \"flow\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "r = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "\n",
    "#print status code\n",
    "print(f\"Status code {r.status_code}\")\n",
    "\n",
    "# if status is success then obtain the workflow id for provisioning in the next step\n",
    "if r.status_code == 200 or r.status_code == 201 :\n",
    "    print(r.text)\n",
    "    workflow_id = json.loads(r.text)[\"workflow_id\"]\n",
    "else: \n",
    "    print(f\"failed {r.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95acc20",
   "metadata": {},
   "source": [
    "### Provision a workflow\n",
    "Following cell will formulate a required prompt for the LLM using parameters and within the prompt it embeds the output of the semantic search results `parameters.output_field:-` to be included so LLM references the information returned in the search results rather than answering from its pre-trained knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5394d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create path to the Workflow provisioning API\n",
    "path = f'_plugins/_flow_framework/workflow/{workflow_id}/_provision'\n",
    "url = f'https://{aos_host}/{path}'\n",
    "print(url)\n",
    "\n",
    "\n",
    "# Let's delete if there is any existing wine_knowledge_base2 index in the system.\n",
    "index_name = \"wine_knowledge_base2\"\n",
    "\n",
    "try:\n",
    "    aos_client.indices.delete(index=index_name)\n",
    "    print(f\"Index {index_name} deleted from the cluster successfully.\")\n",
    "except:\n",
    "    print(\"Index '\" + index_name + \"' not found. Creating index on cluster.\")\n",
    "    \n",
    "    \n",
    "\n",
    "# Create an example for model to learn how to respond to user's question using search results.\n",
    "\n",
    "one_shot_description_example = \"_id: xB2GTpABZyO2331xxdm \\n _source: {'description': 'This perfumey white dances in intense and creamy layers of stone fruit and vanilla, remaining vibrant and balanced from start to finish. The generous fruit is grown in the relatively cooler Oak Knoll section of the Napa Valley. This should develop further over time and in the glass.', 'winery': 'Darioush', 'points': 92, 'designation': None, 'country': 'US'}\"\n",
    "one_shot_response_example = \"I have a wonderful wine for you. It's a dry, medium bodied white wine from Darioush winery in the Oak Knoll section of Napa Valley, US. It has flavors of vanilla and oak. It scored 92 points in wine spectator.\"\n",
    "\n",
    "# System prompt that includes reference to parameters.output_field which indicates inclusion of \n",
    "# the search results.\n",
    "\n",
    "\n",
    "user_prompt = (\n",
    "    f\"[Meta instructions]\"\n",
    "    f\"You must pick one of the wine in \\\"Wine data\\\" section that matches best the user question. Wine data section is in json format.\"\n",
    "    f\"For debugging your results, we need you to output all wine data after your recommendation, in a human readable records format.\\n\"\n",
    "    f\"[Instructions]\"\n",
    "    \"As a sommelier, you must include the wine variety, the country of origin, and a colorful description relating to the user question.  Do not suggest anything outside of the wine data provided. You don't necessarily have to pick the top rated wine if its not best matching user question, but you have to select from within wine data only. If wine data was empty, respond by saying sorry i do not have enough information\\n\"\n",
    "    f\"Example Wine data: {one_shot_description_example} \\n \"\n",
    "    f\"Example recommendation: {one_shot_response_example} \\n\"\n",
    "    \"<Wine data>: ${parameters.output_field:-} \\n\"\n",
    ")\n",
    "\n",
    "# Initializing the flow workflow template parameters that we referenced in the template at the time of \n",
    "# creating the template\n",
    "\n",
    "default_params = {\n",
    "    \"create_ingest_pipeline.pipeline_id\": \"wine-ingest-pipeline2\",\n",
    "    \"text_embedding.field_map.input\": \"description\",\n",
    "    \"text_embedding.field_map.output\": \"description_embedding\",\n",
    "    \"create_index.name\": \"wine_knowledge_base2\",\n",
    "    \"rag_tool.parameters.prompt\": user_prompt,\n",
    "    \"root_agent.parameters.parameters\": \"root_agent.parameters.parameters\",\n",
    "}\n",
    "\n",
    "# Printing the defaul parameters \n",
    "print(json.dumps(default_params, indent=4))\n",
    "\n",
    "# Calling API for provisioning workflow.\n",
    "r = requests.post(url, auth=awsauth, json=default_params, headers=headers)\n",
    "\n",
    "#print status of the API call.\n",
    "print(f\"Status: {r.status_code}. Response:{r.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61c4552",
   "metadata": {},
   "source": [
    "### Check status of the workflow provisioning\n",
    "Above API call initiates the deployment. We will need to check the status if all connectors are deployed successfully using `_status` API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc18d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put together URL to _status API of flow framework\n",
    "path = f'_plugins/_flow_framework/workflow/{workflow_id}/_status'\n",
    "url = f'https://{aos_host}/{path}'\n",
    "print(url)\n",
    "\n",
    "# We will capture various model ids so that we can test them individually if we wanted to.\n",
    "inference_model_id=\"\"\n",
    "embedding_model_id=\"\"\n",
    "agent_id=\"\"\n",
    "index_name = \"\"\n",
    "\n",
    "# Calling the _status API\n",
    "r = requests.get(url, auth=awsauth, json=default_params, headers=headers)\n",
    "\n",
    "print(f\"Status: {r.status_code}.\")\n",
    "\n",
    "# if _status API call returns successfully, we should extract the data we need.\n",
    "if r.status_code == 200: \n",
    "    status = json.loads(r.text)[\"state\"]\n",
    "    if status == \"COMPLETED\":\n",
    "        response_json = json.loads(r.text)\n",
    "        #print(json.dumps( response_json , indent=4))\n",
    "        for resources in response_json[\"resources_created\"]:\n",
    "            if resources[\"workflow_step_id\"] == \"register_bedrock_model\":\n",
    "                inference_model_id=resources[\"resource_id\"]\n",
    "            elif resources[\"workflow_step_id\"] == \"register_bedrock_embedding_model\":\n",
    "                embedding_model_id=resources[\"resource_id\"]    \n",
    "            elif resources[\"workflow_step_id\"] == \"root_agent\":\n",
    "                agent_id=resources[\"resource_id\"]  \n",
    "            elif resources[\"workflow_step_id\"] == \"create_index\":\n",
    "                index_name=resources[\"resource_id\"]          \n",
    "        print(f\"inference model: {inference_model_id}\")\n",
    "        print(f\"Embedding model: {embedding_model_id}\")    \n",
    "        print(f\"Agent id: {agent_id}\")    \n",
    "        print(f\"Index name: {index_name}\")    \n",
    "    else: \n",
    "        print(f\"Failed Response:{r.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfcc890",
   "metadata": {},
   "source": [
    "#### Troubleshooting guidance.\n",
    "\n",
    "If above API call returns a failed status then please check the failure message. If failure is due to a pre-existing index then you may need to delete `wine_knowledge_base2` index or ingestion pipeline. If workflow fails to deploy, please deprovision it, delete index and ingestion pipeline and then attempt to re-run the provisioning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e273785b",
   "metadata": {},
   "source": [
    "## 6. Load the data in index\n",
    "Now that we have setup the index and ingestion pipeline together with models. We are ready to ingest the data. The ingestion will automatically use `wine-ingest-pipeline2` and load data in `wine_knowledge_base2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa804f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "batch = 0\n",
    "action = json.dumps({ \"index\": { \"_index\": index_name } })\n",
    "body_ = ''\n",
    "\n",
    "\n",
    "with alive_bar(len(df_sample), force_tty = True) as bar:\n",
    "    for index, record in (df_sample.iterrows()):\n",
    "\n",
    "        payload={\n",
    "           \"description\": record[\"description\"],\n",
    "           \"points\":record[\"points\"],\n",
    "           \"variety\":record[\"variety\"],\n",
    "           \"country\":record[\"country\"],\n",
    "           \"designation\":record[\"designation\"],\n",
    "           \"winery\":record[\"winery\"]\n",
    "        }\n",
    "        body_ = body_ + action + \"\\n\" + json.dumps(payload) + \"\\n\"\n",
    "        cnt = cnt+1\n",
    "        \n",
    "        if(cnt == 100):\n",
    "            \n",
    "            response = aos_client.bulk(\n",
    "                                index = index_name,\n",
    "                                 body = body_)\n",
    "            cnt = 0\n",
    "            batch = batch +1\n",
    "            body_ = ''\n",
    "        \n",
    "        bar()\n",
    "print(\"Total Bulk batches completed: \"+str(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419b3d86",
   "metadata": {},
   "source": [
    "#### Checking the number of records loaded in index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46b0ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = aos_client.search(index=index_name, body={\"query\": {\"match_all\": {}}})\n",
    "print(\"Records found: %d.\" % res['hits']['total']['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2abe0d8",
   "metadata": {},
   "source": [
    "## 7. Call the agent created by flow framework.\n",
    "We are going to call the Root agent that will call RAG tool to run the RAG workflow that we just created. We will get response from Anthropic claude sonnet 3 model which will reference the search results retrieved from OpenSearch to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3a3330",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Best wine that goes with swine ?\"\n",
    "payload = {\n",
    "    \"parameters\":\n",
    "    { \n",
    "        \"question\": question\n",
    "    } \n",
    "}\n",
    "\n",
    "path = '_plugins/_ml/agents/'+agent_id+'/_execute'\n",
    "url = 'https://'+aos_host + '/' + path\n",
    "r = requests.post(url, auth=awsauth, json=payload , headers=headers )\n",
    "if r.status_code == 200:\n",
    "    print(json.loads(r.text)['inference_results'][0]['output'][0]['result'])\n",
    "else:\n",
    "    print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9a8a08",
   "metadata": {},
   "source": [
    "### Check the question with semantic retrieval \n",
    "You can run a simple semantic search on the created index to compare what records the Claude sonnet referred to answer the question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ca9208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_opensearch_with_semantic_search(phrase, n=3):\n",
    "    osquery={\n",
    "        \"_source\": {\n",
    "            \"exclude\": [ \"description_embedding\" ]\n",
    "        },\n",
    "        \n",
    "      \"size\": n,\n",
    "      \"query\": {\n",
    "        \"neural\": {\n",
    "          \"description_embedding\": {\n",
    "            \"query_text\": f\"{phrase}\",\n",
    "            \"model_id\": f\"{embedding_model_id}\",\n",
    "            \"k\": 5\n",
    "          }\n",
    "        }\n",
    "      }    \n",
    "    }\n",
    "\n",
    "    res = aos_client.search(index=index_name, \n",
    "                           body=osquery,\n",
    "                           stored_fields=[\"description\",\"winery\",\"points\", \"designation\", \"country\"],\n",
    "                           explain = True)\n",
    "    top_result = res['hits']['hits']\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for entry in top_result:\n",
    "        result = {\n",
    "            \"description\":entry['_source']['description'],\n",
    "            \"winery\":entry['_source']['winery'],\n",
    "            \"points\":entry['_source']['points'],\n",
    "            \"designation\":entry['_source']['designation'],\n",
    "            \"country\":entry['_source']['country'],\n",
    "            \"variety\":entry['_source']['variety'],\n",
    "            \"_id\":entry['_id'],\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "retrieve_opensearch_with_semantic_search(question, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3c8d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also test the embedding generation. This model is deployed by the \n",
    "# flow framework as part of the RAG workflow.\n",
    "payload = {\n",
    "    \"parameters\":\n",
    "    { \n",
    "        \"inputText\": question\n",
    "    } \n",
    "}\n",
    "\n",
    "path = '_plugins/_ml/models/'+embedding_model_id+'/_predict'\n",
    "url = 'https://'+aos_host + '/' + path\n",
    "r = requests.post(url, auth=awsauth, json=payload , headers=headers )\n",
    "\n",
    "if r.status_code == 200:\n",
    "    embedding = json.loads(r.text)['inference_results'][0]['output'][0]['data']\n",
    "    print(embedding[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0813986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also call embedding model using ML Common python client.\n",
    "# Both above cell and this cell serve the same purpose.\n",
    "input_sentences = [question]\n",
    "embedding_output = ml_client.generate_embedding(f\"{embedding_model_id}\", input_sentences)\n",
    "embed = embedding_output['inference_results'][0]['output'][0]['data']\n",
    "print(embed[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81207b91",
   "metadata": {},
   "source": [
    "### What did we learn\n",
    "We learn that we can create a complex workflow in OpenSearch and define it with JSON configuration. OpenSearch can deploy these complex workflow with different default parameters so you can create multiple RAG use cases with a single template. OpenSearch service will add more templates in future for you to develop semantic search, hybrid search and other generative AI use cases quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ba8bcf",
   "metadata": {},
   "source": [
    "## Deprovision a workflow\n",
    "\n",
    "After you have tested the above with multiple different questions you may want to deprovision a workflow. When you deprovision a workflow it would undeploy all the model connections that it originally created. To be able to delete workflow you will first need to deprovision it. Following code shows how to deprovision a workflow. \n",
    "\n",
    "*Note that any index resource that it creates will not be deleted. Our `wine_knowledgebase2` index won't be deleted.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c614df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create path to deprovisioning API}\n",
    "path = f'_plugins/_flow_framework/workflow/{workflow_id}/_deprovision'\n",
    "url = f'https://{aos_host}/{path}'\n",
    "print(url)\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "r = requests.post(url, auth=awsauth, json=default_params, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fee333",
   "metadata": {},
   "source": [
    "## Delete a workflow\n",
    "Deprovisioning a workflow leaves the template behind. You will need to delete the workflow for it to be removed from the template. Template does not occupy much space, however, for proper clean up you will need to delete the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c79ab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create URL that we will use with DELETE HTTP method.\n",
    "path = f'_plugins/_flow_framework/workflow/{workflow_id}'\n",
    "url = f'https://{aos_host}/{path}'\n",
    "print(url)\n",
    "\n",
    "\n",
    "r = requests.delete(url, auth=awsauth, json=default_params, headers=headers)\n",
    "print(r.status_code)\n",
    "print(r.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
