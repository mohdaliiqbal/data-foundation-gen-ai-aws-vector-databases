{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afb05169",
   "metadata": {},
   "source": [
    "# Text Chunking\n",
    "This lab will walk you through various methods to perform chunking of your text. Retrieval is a very important step in RAG architecture. Semantic search requires you take your knowledge/text and convert that into embeddings and store them in a search engine that offers vector search capability. To convert your documents into embedding, you will need to split them into smaller pieces, popularly called \"Chunks\". This technique is known as \"Chunking\". Chunking is necessary because a large text passage may reduce its specificity, it may conflate different topics or concepts making it not best match for a query about a topic. This would mean even if there is a very relevant information in one part of the text passage, the similarity of text passage as a whole to user's query may be low, this may exclude the text passage from top semantic search results.\n",
    "\n",
    "\n",
    "There is a [great resource by Greg Kamradt](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb) from where you can learn about various ways to chunk text  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218fb6d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain langchain_community pypdf langchain_experimental --quiet\n",
    "!pip install -qU langchain-text-splitters\n",
    "!pip install --upgrade --quiet  boto3\n",
    "!pip install pdfminer.six --quiet\n",
    "!pip install amazon-textract-caller --quiet\n",
    "!pip install amazon-textract-textractor --quiet\n",
    "!pip install opensearchpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1915eec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sagemaker\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from sagemaker import get_execution_role\n",
    "import random \n",
    "import string\n",
    "import s3fs\n",
    "from urllib.parse import urlparse\n",
    "from IPython.display import display, HTML\n",
    "from alive_progress import alive_bar\n",
    "from opensearch_py_ml.ml_commons import MLCommonClient\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import requests \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87760ccf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Boto3 session\n",
    "session = boto3.Session()\n",
    "\n",
    "# Get the account id\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# Get the current region\n",
    "region = session.region_name\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "\n",
    "# Method to obtain output variables from Cloudformation stack. \n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "## Setup variables to for the rest of the demo\n",
    "cloudformation_stack_name = \"genai-data-foundation-workshop\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "aos_host = outputs['OpenSearchDomainEndpoint']\n",
    "s3_bucket = outputs['s3BucketTraining']\n",
    "bedrock_inf_iam_role = outputs['BedrockBatchInferenceRole']\n",
    "bedrock_inf_iam_role_arn = outputs['BedrockBatchInferenceRoleArn']\n",
    "sagemaker_notebook_url = outputs['SageMakerNotebookURL']\n",
    "\n",
    "# We will just print all the variables so you can easily copy if needed.\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3773f353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "\n",
    "def upload_file_to_s3(file_path, bucket_name, prefix):\n",
    "    s3 = boto3.client('s3')\n",
    "    file_name = os.path.basename(file_path)\n",
    "    object_key = f\"{prefix}/{file_name}\" if prefix else file_name\n",
    "    \n",
    "    with open(file_path, 'rb') as file_data:\n",
    "        s3.upload_fileobj(file_data, bucket_name, object_key)\n",
    "    \n",
    "    s3_path = f\"s3://{bucket_name}/{object_key}\"\n",
    "    return s3_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a86734",
   "metadata": {},
   "source": [
    "### Lang chain recursive character chunking\n",
    "The most simplest way to chunk document would be by length, but keeping paragraphs or lines together so it does not lose the meaning. We will use langchain library's recursive character text splitter which offers ways to split data by length, yet keeps the lines, paragraph together as much as possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a82f2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this method would split the text into chunks by paragraph, line boundary and keeping chunk \n",
    "# size as close to 1000 characters, it will also overlap the text between chunks if it were to \n",
    "# split line or paragraph in the middle.\n",
    "\n",
    "def recursive_character_chunking(text): \n",
    "    \n",
    "#     embeddings = BedrockEmbeddings() #create a Titan Embeddings client\n",
    "    \n",
    "#     pdf_path = \"Amazon-com-Inc-2023-Annual-Report.pdf\" #assumes local PDF file with this name\n",
    "\n",
    "#     loader = PyPDFLoader(file_path=pdf_path) #load the pdf file\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter( #create a text splitter\n",
    "        #separators=[\"\\n\\n\", \"\\n\", \".\", \" \"], #split chunks at (1) paragraph, (2) line, (3) sentence, or (4) word, in that order\n",
    "        chunk_size=1000, #divide into 1000-character chunks using the separators above\n",
    "        chunk_overlap=100, #number of characters that can overlap with previous chunk\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    \n",
    "    docs = text_splitter.create_documents(text)#From the loaded PDF\n",
    "    \n",
    "    return docs #return the index to be cached by the client app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519b894d",
   "metadata": {},
   "source": [
    "Let's try to run this method on an excerpt of AWS docs from Amazon Bedrock titan model and Amazon Textract services. You will notice that length/recursive chunking will create chunks with overlaps, this helps in situations where sentences need to not be chopped in the middle, but it will fail to keep Textract and Titan documentation chunks separate. You will notice that chunk no. 9 is a mix of titan and lambda docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237acdf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "\n",
    "#lets load text from our prepared aws-docs-excerpt from various services.\n",
    "with open('aws-docs-excerpt.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "\n",
    "docs = recursive_character_chunking([text])\n",
    "\n",
    "# the method prints chunks\n",
    "def print_chunks(data):\n",
    "    #Let's print the chunks -- notice the overlap between chunk 3 and 4\n",
    "    i = 1\n",
    "    for doc in data:\n",
    "        print(f\"---------START OF CHUNK {i}------\")\n",
    "        print(f\"{doc.page_content}\")\n",
    "        print(f\"---------END OF CHUNK {i}------\\n\\n\")\n",
    "        i+=1\n",
    "        \n",
    "print_chunks(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6519d5f2-9c1d-4a9e-8d44-ee0b1e060486",
   "metadata": {},
   "source": [
    "## PDF Parsing\n",
    "We have to convert our files into a String object before we can perform any form of chunking strategy. Here, we will parse our PDF into a string using PyPDFLoader from Langchain. This library will attempt to retain the format of the texts as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad33dfd3-1728-491c-9c2c-db189d3b5b5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"Amazon-com-Inc-2023-Annual-Report.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "#print(documents)\n",
    "\n",
    "texts = \"\"\n",
    "\n",
    "for document in documents:\n",
    "    texts += document.page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb3cc9d",
   "metadata": {},
   "source": [
    "### Semantic chunking\n",
    "Semantic chunking is a novel technique that chunks the data in a way that it optimises it for semantic cohesion. The method uses an embedding model and runs similarity calculation over sentences and decides the chunk position based on deviation/change in semantic distance between sentences. It uses rolling window where it keeps adding sentences and measure its distance with incoming sentence. Technically a change in topic should be detected (not very accurately). A breakpoint threshold is statistical method use to determine this change. This way you ensure that chunks stay optimal for semantic matching. \n",
    "\n",
    "If you are keen to get more info, read level 4 in [Greg Kamradt tutorial](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb).\n",
    "\n",
    "Langchain offers semantic chunking and also ability to call embedding model. We will first choose an embedding model for our semantic chunking a a breakpoint threshold type. After selecting the model and threshold, please move to the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67023018",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#lets initialize the code for drop down box input.\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interactive\n",
    "\n",
    "#defaults\n",
    "model_id='amazon.titan-embed-text-v2:0'\n",
    "threshold = 'percentile'\n",
    "\n",
    "#list of embedding models in bedrock\n",
    "model_list=['cohere.embed-english-v3','cohere.embed-multilingual-v3',\n",
    "            'amazon.titan-embed-text-v1','amazon.titan-embed-text-v2:0',\n",
    "           'amazon.titan-embed-image-v1']\n",
    "\n",
    "#semantic chunking \n",
    "threshold_list=['percentile', 'standard_deviation', 'interquartile']\n",
    "    \n",
    "drop1 = widgets.Dropdown(options=model_list, value='cohere.embed-english-v3', description='Model:', disabled=False)\n",
    "drop2 = widgets.Dropdown(options=threshold_list, value='percentile', description='Threshold:', disabled=False)\n",
    "\n",
    "def get_model_dimension(model_id):\n",
    "    if model_id==\"amazon.titan-embed-text-v2:0\":\n",
    "        return 1024\n",
    "    if model_id.startswith(\"cohere\"):\n",
    "        return 512\n",
    "    if model_id.startswith(\"amazon.titan-embed-text-v1\"):\n",
    "        return 8192\n",
    "    if model_id.startswith(\"amazon.titan-embed-image-v1\"):\n",
    "        return 8192"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a066d49",
   "metadata": {},
   "source": [
    "Following code runs semantic chunking for aws docs text. It also shows a drop down for you to change the model and threshold type so you can see the effects of various models and breakpoint thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea9c0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "#from langchain_community.embeddings import BedrockEmbedding\n",
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "\n",
    "#method that is called when drop down boxes are shown or changed\n",
    "def update_dropdown(selected_model, selected_threshold):\n",
    "    model_id = selected_model.lower()\n",
    "    threshold = selected_threshold.lower()\n",
    "    info = f\"Selected embedding model: {model_id}. Selected threshold: {threshold}!\"\n",
    "    display(info)\n",
    "    semantic_chunks = perform_semantic_chunking(text=text, model_id=model_id, threshold=threshold)\n",
    "    print_chunks(semantic_chunks)\n",
    "\n",
    "    \n",
    "# method runs semantic chunking on text for a given model and threshold.    \n",
    "def perform_semantic_chunking(text, model_id, threshold):\n",
    "    print(f\"Chunking using {model_id} and {threshold} threshold breaking point\")\n",
    "    \n",
    "    #using lang chain's Bedrock embedding object\n",
    "    embeddings = BedrockEmbeddings(region_name=region, model_id=model_id)\n",
    "\n",
    "    #using lang chain's semantic chunker to chunk\n",
    "    text_splitter = SemanticChunker(\n",
    "        embeddings, breakpoint_threshold_type= threshold\n",
    "    )\n",
    "\n",
    "    docs = text_splitter.create_documents([text])\n",
    "    return docs\n",
    "\n",
    "#lets run semantic chunking and display the drop down. \n",
    "w = interactive(update_dropdown, selected_model=drop1, selected_threshold=drop2) \n",
    "display(w)\n",
    "\n",
    "#when you change value - give it takes a 10-15 seconds for refreshing the chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ae5ea4",
   "metadata": {},
   "source": [
    "You can run above cell a number of with different combination of the embedding model and threshold ids to see what breaks the content best. You will find results vary from one model to another and between various breakpoint threshold technique. However, it does not mean this combination will always be best for chunking. Note that this also does not mean it is optimal for retrieval. We will have to test this with our queries to know if this is best to answer our questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2a0e8a",
   "metadata": {},
   "source": [
    "### Loading text chunks in opensearch to run semantic search over chunks\n",
    "\n",
    "#### Let's first create an index with KNN field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2f5354",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_index = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True,\n",
    "        \"index.knn.space_type\": \"cosinesimil\",\n",
    "        \"analysis\": {\n",
    "          \"analyzer\": {\n",
    "            \"default\": {\n",
    "              \"type\": \"standard\",\n",
    "              \"stopwords\": \"_english_\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"chunk_vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": get_model_dimension(model_id=model_id),\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"chunk_content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8c1c54",
   "metadata": {},
   "source": [
    "### Connect to Amazon OpenSearch Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2d3ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kms = boto3.client('secretsmanager')\n",
    "aos_credentials = json.loads(kms.get_secret_value(SecretId=outputs['OpenSearchSecret'])['SecretString'])\n",
    "\n",
    "# For this lab we will use credentials that we have already created in AWS Secrets manager service. Secrets\n",
    "# manager service allows you to store secrets securily and retrieve it through code in a safe manner.\n",
    "\n",
    "auth = (aos_credentials['username'], aos_credentials['password'])\n",
    "\n",
    "aos_client = OpenSearch(\n",
    "    hosts = [{'host': aos_host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5424188e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"aws_docs_index\"\n",
    "\n",
    "try:\n",
    "    aos_client.indices.delete(index=index_name)\n",
    "    print(\"Recreating index '\" + index_name + \"' on cluster.\")\n",
    "    aos_client.indices.create(index=index_name,body=knn_index,ignore=400)\n",
    "except:\n",
    "    print(\"Index '\" + index_name + \"' not found. Creating index on cluster.\")\n",
    "    aos_client.indices.create(index=index_name,body=knn_index,ignore=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4b3b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import embed_phrase, opensearch_bulk_load\n",
    "\n",
    "#test calling embed_phrase method from utilities file to get embedding of a given model.\n",
    "embed_phrase(\"Testing amazon bedrock models\", model_id, bedrock_client=bedrock_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861ff02b",
   "metadata": {},
   "source": [
    "### Loading data in to opensearch\n",
    "Let's load data into opensearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe8ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process all the chunks and get embeddings from Bedrock for each text chunk\n",
    "chunks = []\n",
    "\n",
    "for doc in docs:\n",
    "    chunks.append({\"chunk_content\": doc.page_content, \"chunk_vector\": embed_phrase(doc.page_content, model_id, bedrock_client)})\n",
    "\n",
    "print(chunks[0])\n",
    "\n",
    "#load data into opensearch - every chunk will be separate opensearch record/document.\n",
    "opensearch_bulk_load(chunks, index_name, aos_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d683e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_opensearch_with_semantic_search(phrase, model_id, bedrock_client, n=3 ):\n",
    "    search_vector = embed_phrase(phrase, model_id=model_id, bedrock_client=bedrock_client)\n",
    "    osquery={\n",
    "        \"_source\": {\n",
    "            \"exclude\": [ \"chunk_vector\" ]\n",
    "        },\n",
    "        \n",
    "      \"size\": n,\n",
    "      \"query\": {\n",
    "        \"knn\": {\n",
    "          \"chunk_vector\": {\n",
    "            \"vector\":search_vector,\n",
    "            \"k\":n\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "\n",
    "    res = aos_client.search(index=index_name, \n",
    "                           body=osquery,\n",
    "                           stored_fields=[\"chunk_content\"],\n",
    "                           explain = True)\n",
    "    top_result = res['hits']['hits']\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for entry in top_result:\n",
    "        result = {\n",
    "            \"chunk_content\":entry['_source']['chunk_content'],\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f994693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_on_s3_docs=\"What VPN connections do ?\"\n",
    "\n",
    "example_request = retrieve_opensearch_with_semantic_search(phrase=question_on_s3_docs, model_id=model_id, bedrock_client=bedrock_client, n=2)\n",
    "print(json.dumps(example_request, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5715be81-17f3-4691-81e8-2cee20443fc4",
   "metadata": {},
   "source": [
    "## Building a RAG pipeline with Langchain\n",
    "Work in Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576d4b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\" : retrieve_opensearch_with_semantic_search, \"question\" : RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
